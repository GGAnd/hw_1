{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "j0epe6PM-7H2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c052142-ffe8-43f0-a768-e803df7cb975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import re\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense,  GlobalMaxPooling1D, LSTM\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout, Activation, Flatten\n",
        "from keras.models import Model\n",
        "from keras.initializers import Constant\n",
        "from keras.models import Sequential\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import *\n",
        "\n",
        "word_stemmer = PorterStemmer()\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF819XpiFUTE",
        "outputId": "db515777-7fe4-4b71-93a5-1c48f65c9510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка данных"
      ],
      "metadata": {
        "id": "G1PdGsaODtEJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qcz-IKjtCd4k"
      },
      "outputs": [],
      "source": [
        "real_news_df = pd.read_csv('drive/MyDrive/dataset/True.csv')\n",
        "fake_news_df = pd.read_csv('drive/MyDrive/dataset/Fake.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uFZ8QmsWFyAq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e46c3c3-9ad8-4a13-9b3d-5175c025cc78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-f048deb7a624>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  real_news_df['real_fact'] = 1\n",
            "<ipython-input-15-f048deb7a624>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  fake_news_df['real_fact'] = 0\n"
          ]
        }
      ],
      "source": [
        "real_news_df = real_news_df[real_news_df['text'].str.len() >= 3]\n",
        "fake_news_df = fake_news_df[fake_news_df['text'].str.len() >=3]\n",
        "real_news_df['real_fact'] = 1\n",
        "fake_news_df['real_fact'] = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Очистка данных.\n",
        "\n",
        "\n",
        "\n",
        "*   Функция decontracted используется для расширения сокращенных слов в данной текстовой фразе.\n",
        "*  Функция get_cleaned_data принимает входные данные (текстовые данные) и выполняет над ними несколько шагов очистки данных.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JFiT3y5rFlUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decontracted(phrase):\n",
        "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "def get_cleaned_data(input_data, mode='df'):\n",
        "    stop = stopwords.words('english')\n",
        "    input_df = ''\n",
        "    if mode != 'df':\n",
        "        input_df = pd.DataFrame([input_data], columns=['text'])\n",
        "    else:\n",
        "        input_df = input_data\n",
        "    input_df['text'] = input_df['text'].str.lower()\n",
        "    input_df['text'] = input_df['text'].apply(lambda elem: decontracted(elem))\n",
        "    input_df['text'] = input_df['text'].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))\n",
        "    input_df['text'] = input_df['text'].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
        "    input_df['text'] = input_df['text'].apply(lambda x: ' '.join([word.strip() for word in x.split() if word not in (stop)]))\n",
        "    input_df['text'] = input_df['text'].apply(lambda words: (wordnet_lemmatizer.lemmatize(words)))\n",
        "    return input_df\n",
        "\n",
        "fake_news_df = get_cleaned_data(fake_news_df)\n",
        "real_news_df = get_cleaned_data(real_news_df)\n",
        "#объединение очищенные данные в один news_data_df.\n",
        "news_data_df = pd.concat([real_news_df, fake_news_df], ignore_index = True)\n",
        "print(news_data_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOagYctwucLP",
        "outputId": "8ddb2bb2-0501-4e86-fcf3-b5d3840adb1a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(44267, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделение данных на обучающий и тестовый наборы с использованием функции train_test_split"
      ],
      "metadata": {
        "id": "5jeeKoWxQmEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 500  # Максимальная длина последовательности\n",
        "MAX_NUM_WORDS = 10000  # Максимальное количество слов\n",
        "EMBEDDING_DIM = 300  # Размерность вектора слов\n",
        "VALIDATION_SPLIT = 0.3  # Доля данных для валидации\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(news_data_df.text,news_data_df.real_fact,random_state = 42, test_size=VALIDATION_SPLIT, shuffle=True)"
      ],
      "metadata": {
        "id": "ChiNzbzkug6G"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Векторизовать образцы текста в двумерный целочисленный тензор"
      ],
      "metadata": {
        "id": "JofbjhRfEs8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
        "\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "tokenized_train = tokenizer.texts_to_sequences(x_train)\n",
        "X_train = pad_sequences(tokenized_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found {} unique tokens. and {} lines '.format(len(word_index), len(X_train)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwkGQFpLurqi",
        "outputId": "a88a3ed5-0f33-4b2a-b1f0-ae6e54fd80bc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 169780 unique tokens. and 30986 lines \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_test = tokenizer.texts_to_sequences(x_test)\n",
        "X_test = pad_sequences(tokenized_test, maxlen=MAX_SEQUENCE_LENGTH)"
      ],
      "metadata": {
        "id": "gASPjOvnu4G9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получите предварительно обученный индекс внедрения из GoogleNews-vectors-negative300:\n",
        "\n",
        "Векторы GoogleNews располагаются в порядке от наиболее частого к наименее частому, поэтому первые N обычно представляют собой подмножество размера N.\n",
        "Поэтому используйте limit=500000, чтобы получить наиболее часто встречающиеся векторы из 500 000 слов, экономя 5/6 памяти/времени загрузки."
      ],
      "metadata": {
        "id": "BWz-zXPEMNTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "def get_embeddings(path):\n",
        "  wv_from_bin = KeyedVectors.load_word2vec_format(path, binary=True, limit=500000)\n",
        "  embeddings_index = {}\n",
        "  for word, vector in zip(wv_from_bin.key_to_index, wv_from_bin.vectors):\n",
        "      coefs = np.asarray(vector, dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "  return embeddings_index\n",
        "\n",
        "embeddings_index = {}\n",
        "embeddings_index = get_embeddings('drive/MyDrive/dataset/GoogleNews-vectors-negative300.bin')\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFpWkN77u8JV",
        "outputId": "37ccf1be-69fc-40e0-c427-9f5795824acf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 500000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создания матрицы встраивания для модели нейронной сети."
      ],
      "metadata": {
        "id": "PizSfnkFRp9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    try:\n",
        "        embedding_vector = embeddings_index[word]\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "    except KeyError:\n",
        "        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)"
      ],
      "metadata": {
        "id": "86kvk8Ea0K8r"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del embeddings_index"
      ],
      "metadata": {
        "id": "GDAkaLwv1wwu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подготовьте модель CNN с помощью GlobalMaxPooling для классификации."
      ],
      "metadata": {
        "id": "Rv_7GLG2MjHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_net1():\n",
        "    model = Sequential()\n",
        "\n",
        "    #Non-trainable embeddidng layer\n",
        "    model.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
        "\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Conv1D(filters=128, kernel_size=4, activation='relu'))\n",
        "    model.add(GlobalMaxPooling1D())\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units = 250 , activation = 'relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "EDjDaKDr1yGb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подготовьте модель LSTM"
      ],
      "metadata": {
        "id": "k_Pm3EL2MyNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_net1():\n",
        "    model = Sequential()\n",
        "\n",
        "    #Non-trainable embeddidng layer\n",
        "    model.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
        "\n",
        "    model.add(LSTM(units=128 , return_sequences = True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=64))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(units = 32 , activation = 'relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "l_ofn_YY15iB"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Устанавливаем batch_size = 256,epochs = 8\n"
      ],
      "metadata": {
        "id": "uAugcKMHNAUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = cnn_net1()\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 8\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bMvv_Fjy2dXn",
        "outputId": "41a34f42-ae2c-4c45-b843-361d3e32a37a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 500, 300)          50934300  \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 500, 300)          0         \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 497, 128)          153728    \n",
            "                                                                 \n",
            " global_max_pooling1d (Glob  (None, 128)               0         \n",
            " alMaxPooling1D)                                                 \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 250)               32250     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 251       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51120529 (195.01 MB)\n",
            "Trainable params: 186229 (727.46 KB)\n",
            "Non-trainable params: 50934300 (194.30 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yz-nl5KJ2jKh",
        "outputId": "d2a9673e-917e-4c25-f6d3-3a18b3cc8a8a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "122/122 [==============================] - 9s 46ms/step - loss: 0.2297 - accuracy: 0.8972 - val_loss: 0.0256 - val_accuracy: 0.9922\n",
            "Epoch 2/8\n",
            "122/122 [==============================] - 5s 41ms/step - loss: 0.0330 - accuracy: 0.9894 - val_loss: 0.0212 - val_accuracy: 0.9946\n",
            "Epoch 3/8\n",
            "122/122 [==============================] - 6s 45ms/step - loss: 0.0215 - accuracy: 0.9930 - val_loss: 0.0148 - val_accuracy: 0.9957\n",
            "Epoch 4/8\n",
            "122/122 [==============================] - 6s 46ms/step - loss: 0.0167 - accuracy: 0.9944 - val_loss: 0.0131 - val_accuracy: 0.9959\n",
            "Epoch 5/8\n",
            "122/122 [==============================] - 5s 41ms/step - loss: 0.0122 - accuracy: 0.9960 - val_loss: 0.0122 - val_accuracy: 0.9964\n",
            "Epoch 6/8\n",
            "122/122 [==============================] - 6s 46ms/step - loss: 0.0092 - accuracy: 0.9968 - val_loss: 0.0172 - val_accuracy: 0.9960\n",
            "Epoch 7/8\n",
            "122/122 [==============================] - 6s 47ms/step - loss: 0.0068 - accuracy: 0.9981 - val_loss: 0.0122 - val_accuracy: 0.9965\n",
            "Epoch 8/8\n",
            "122/122 [==============================] - 6s 46ms/step - loss: 0.0068 - accuracy: 0.9975 - val_loss: 0.0131 - val_accuracy: 0.9970\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оценка производительности на обучающем и тестовом наборах данных."
      ],
      "metadata": {
        "id": "5I72ilt6V0xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accr_train = model.evaluate(X_train,y_train)\n",
        "print('Accuracy Train: {}'.format(accr_train[1]*100))\n",
        "accr_test = model.evaluate(X_test,y_test)\n",
        "print('Accuracy Test: {}'.format(accr_test[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-iyEiAg6k3G",
        "outputId": "2102b0e6-fbb8-4256-e980-e25422d45f38"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "969/969 [==============================] - 4s 4ms/step - loss: 4.6621e-04 - accuracy: 0.9999\n",
            "Accuracy Train: 99.99354481697083\n",
            "416/416 [==============================] - 2s 6ms/step - loss: 0.0131 - accuracy: 0.9970\n",
            "Accuracy Test: 99.69881772994995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = lstm_net1()\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 8\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAvDkHiGelLE",
        "outputId": "432dff61-4a84-48bd-f269-aeeda5c34e2b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 500, 300)          50934000  \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 500, 128)          219648    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 500, 128)          0         \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 64)                49408     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51205169 (195.33 MB)\n",
            "Trainable params: 271169 (1.03 MB)\n",
            "Non-trainable params: 50934000 (194.30 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X_train, y_train, batch_size = batch_size , validation_data = (X_test,y_test) , epochs = epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3s7C1jyeors",
        "outputId": "34c898c0-0001-4ae5-a0d7-552823ab10b1"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "122/122 [==============================] - 21s 119ms/step - loss: 0.2431 - accuracy: 0.9079 - val_loss: 0.1048 - val_accuracy: 0.9691\n",
            "Epoch 2/8\n",
            "122/122 [==============================] - 13s 106ms/step - loss: 0.2460 - accuracy: 0.8967 - val_loss: 0.1438 - val_accuracy: 0.9514\n",
            "Epoch 3/8\n",
            "122/122 [==============================] - 13s 107ms/step - loss: 0.1516 - accuracy: 0.9461 - val_loss: 0.0925 - val_accuracy: 0.9667\n",
            "Epoch 4/8\n",
            "122/122 [==============================] - 13s 108ms/step - loss: 0.0610 - accuracy: 0.9820 - val_loss: 0.0296 - val_accuracy: 0.9925\n",
            "Epoch 5/8\n",
            "122/122 [==============================] - 14s 116ms/step - loss: 0.0338 - accuracy: 0.9914 - val_loss: 0.0164 - val_accuracy: 0.9950\n",
            "Epoch 6/8\n",
            "122/122 [==============================] - 13s 110ms/step - loss: 0.0130 - accuracy: 0.9961 - val_loss: 0.0119 - val_accuracy: 0.9966\n",
            "Epoch 7/8\n",
            "122/122 [==============================] - 14s 112ms/step - loss: 0.0113 - accuracy: 0.9967 - val_loss: 0.0116 - val_accuracy: 0.9967\n",
            "Epoch 8/8\n",
            "122/122 [==============================] - 15s 120ms/step - loss: 0.0079 - accuracy: 0.9977 - val_loss: 0.0109 - val_accuracy: 0.9968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оценка производительности на обучающем и тестовом наборах данных."
      ],
      "metadata": {
        "id": "cgHGzhNGV9no"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accr_train = model.evaluate(X_train,y_train)\n",
        "print('Accuracy Train: {}'.format(accr_train[1]*100))\n",
        "accr_test = model.evaluate(X_test,y_test)\n",
        "print('Accuracy Test: {}'.format(accr_test[1]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rauiXqpDescq",
        "outputId": "4d95abed-3a02-4351-c611-a957b87a558f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "969/969 [==============================] - 14s 15ms/step - loss: 0.0050 - accuracy: 0.9988\n",
            "Accuracy Train: 99.87736344337463\n",
            "416/416 [==============================] - 6s 15ms/step - loss: 0.0109 - accuracy: 0.9968\n",
            "Accuracy Test: 99.68376159667969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H9xf5VTIOH-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Все модели с предварительно обученным Word2Vec от GoogleNewsVectors.\n",
        "\n",
        "*   с GlobalMaxpool 99%\n",
        "*   LSTM: 99%\n",
        "\n",
        "\n",
        "Использование Google News Vectors в качестве входных данных для модели CNN и LSTM позволяет улучшить качество классификации фейковых новостей за счет более точного представления слов и их значений. Кроме того, комбинация CNN и LSTM позволяет модели извлекать как локальные, так и глобальные признаки из текста, что способствует лучшему пониманию контекста и выявлению признаков, характеризующих фейковые новости.\n",
        "\n"
      ],
      "metadata": {
        "id": "zo5IPLH6N8EY"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
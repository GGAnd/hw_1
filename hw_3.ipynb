{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "6rg3VyHwtxq1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка ресурсов NLTK для стоп-слов и токенизаци"
      ],
      "metadata": {
        "id": "4IOHh2BvzZIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R6bO7xEyM6y",
        "outputId": "3693aa5c-734d-49a6-bac5-cbbb13abcf01"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка данных"
      ],
      "metadata": {
        "id": "Ku2LSeyUzQG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"spam_or_not_spam.csv\", header=None, names=['email', 'label'])"
      ],
      "metadata": {
        "id": "8RAQm9_QyNW9"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Замена,Токенизация текста,Удаление стоп-слов и пунктуации"
      ],
      "metadata": {
        "id": "kRZe_AMazaoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):  # Проверяем, является ли значение строкой\n",
        "        # Замена \"NUMBER\" на \"number\" и \"URL\" на \"url\"\n",
        "        text = text.replace(\"NUMBER\", \"number\")\n",
        "        text = text.replace(\"URL\", \"url\")\n",
        "\n",
        "        # Токенизация текста\n",
        "        tokens = word_tokenize(text.lower())\n",
        "\n",
        "        # Удаление стоп-слов и пунктуации\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [token for token in tokens if token not in stop_words and token not in string.punctuation]\n",
        "\n",
        "        return tokens\n",
        "    else:\n",
        "        return []  # Возвращаем пустой список для значений, не являющихся строками"
      ],
      "metadata": {
        "id": "VC5WVH99yNp4"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Применение предобработки к столбцу с электронными письмами"
      ],
      "metadata": {
        "id": "5ODHNgMbzzSd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['clean_text'] = data['email'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "pZEafFCayN6E"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделение данных на обучающий и тестовый наборы"
      ],
      "metadata": {
        "id": "S4RU7d7oz2Lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data['clean_text'], data['label'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "VeNCBCKuyOdI"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение модели"
      ],
      "metadata": {
        "id": "KpXl4nNGz6YM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = Word2Vec(sentences=X_train, vector_size=100, window=5, sg=1, min_count=1)\n",
        "ft_model = FastText(sentences=X_train, vector_size=100, window=5, min_count=1, sg=1)"
      ],
      "metadata": {
        "id": "8_NxuKCHyfSV"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оценка векторных представлений с использованием методов most_similar и doesnt_match"
      ],
      "metadata": {
        "id": "YAmjoYQL0C7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_embeddings(model):\n",
        "    print(\"Word Similarity:\")\n",
        "    print(model.wv.most_similar(\"email\"))\n",
        "    print(model.wv.most_similar(\"spam\"))\n",
        "    try:\n",
        "        print(model.wv.most_similar(\"not\"))\n",
        "    except KeyError:\n",
        "        print(\"'not' not present in vocabulary\")\n",
        "\n",
        "    print(\"\\nDoesn't Match:\")\n",
        "    print(model.wv.doesnt_match([\"email\", \"message\", \"phone\"]))\n",
        "    print(model.wv.doesnt_match([\"spam\", \"junk\", \"ham\"]))\n",
        "    try:\n",
        "        print(model.wv.doesnt_match([\"not\", \"no\", \"yes\"]))\n",
        "    except KeyError:\n",
        "        print(\"'not' not present in vocabulary\")"
      ],
      "metadata": {
        "id": "gI0qhdFIygej"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оценка векторных представлений для модели Word2Vec"
      ],
      "metadata": {
        "id": "TltAdDuT0JEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Word2Vec Evaluation:\")\n",
        "evaluate_embeddings(w2v_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMuBuDu8yk4m",
        "outputId": "a602c193-1ce4-40f8-eabc-87dcbdccd35a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.keyedvectors:vectors for words {'not', 'no'} are not present in the model, ignoring these words\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec Evaluation:\n",
            "Word Similarity:\n",
            "[('watching', 0.7851579785346985), ('assignee', 0.7700648307800293), ('sponsored', 0.7651469707489014), ('dump', 0.764888346195221), ('blank', 0.7553474307060242), ('sincerely', 0.7539013624191284), ('opt', 0.749565839767456), ('opted', 0.7454697489738464), ('replying', 0.7443695068359375), ('promotional', 0.7437946200370789)]\n",
            "[('ham', 0.8581007122993469), ('corpus', 0.8067672848701477), ('nonspam', 0.784724235534668), ('filtering', 0.7835919857025146), ('headers', 0.7724446654319763), ('setnumber', 0.7721927165985107), ('score', 0.7707676887512207), ('trained', 0.769098162651062), ('training', 0.7622032165527344), ('filters', 0.761142373085022)]\n",
            "'not' not present in vocabulary\n",
            "\n",
            "Doesn't Match:\n",
            "message\n",
            "junk\n",
            "yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оценка векторных представлений для модели fastText"
      ],
      "metadata": {
        "id": "COUp3szb0WIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nFastText Evaluation:\")\n",
        "evaluate_embeddings(ft_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWn2ovmwyom_",
        "outputId": "00422ddb-be80-4bfc-855c-84b362367ebd"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "FastText Evaluation:\n",
            "Word Similarity:\n",
            "[('emails', 0.9407628774642944), ('bmail', 0.9314588308334351), ('emailers', 0.9310613870620728), ('emailed', 0.9287028312683105), ('orig_email', 0.9281359910964966), ('omail', 0.9236548542976379), ('webmail', 0.9187140464782715), ('nomoremail', 0.9185559749603271), ('qmail', 0.9185280203819275), ('rmail', 0.917107343673706)]\n",
            "[('spamm', 0.9552136659622192), ('spammy', 0.9550026059150696), ('spamcop', 0.9300490021705627), ('spaß', 0.9292759299278259), ('spamtrap', 0.9282188415527344), ('spamc', 0.9254674315452576), ('spamhams', 0.9242157340049744), ('nospam', 0.921402096748352), ('spamd', 0.9170072674751282), ('nonspam', 0.9168686270713806)]\n",
            "[('logwatch', 0.9762375950813293), ('butt', 0.9741703271865845), ('login', 0.9731600284576416), ('autopsy', 0.9726884961128235), ('snot', 0.9720602631568909), ('_not_', 0.9702712893486023), ('autopc', 0.9692462086677551), ('notch', 0.9687154293060303), ('whit', 0.9665449261665344), ('patch', 0.9655031561851501)]\n",
            "\n",
            "Doesn't Match:\n",
            "message\n",
            "junk\n",
            "yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Преобразование текста в векторы с использованием Word2Vec"
      ],
      "metadata": {
        "id": "Uv255rrz0rhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_vectors(model, text):\n",
        "    vectors = [model.wv[word] for word in text if word in model.wv]\n",
        "    return np.mean(vectors, axis=0) if vectors else np.zeros(model.vector_size)"
      ],
      "metadata": {
        "id": "g8yeUtuRyx_n"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Преобразование текста в векторы для обучающего и тестового наборов"
      ],
      "metadata": {
        "id": "q989QYnx0v-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_w2v = np.array([text_to_vectors(w2v_model, text) for text in X_train])\n",
        "X_test_w2v = np.array([text_to_vectors(w2v_model, text) for text in X_test])\n",
        "\n",
        "X_train_ft = np.array([text_to_vectors(ft_model, text) for text in X_train])\n",
        "X_test_ft = np.array([text_to_vectors(ft_model, text) for text in X_test])"
      ],
      "metadata": {
        "id": "a2ko6SPcy1qs"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инициализация и обучение модели логистической регрессии на векторах"
      ],
      "metadata": {
        "id": "HVXFq48901l-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_model_w2v = LogisticRegression()\n",
        "lr_model_w2v.fit(X_train_w2v, y_train)\n",
        "\n",
        "lr_model_ft = LogisticRegression()\n",
        "lr_model_ft.fit(X_train_ft, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "4wHCX7vZy5sz",
        "outputId": "15d76181-3105-44ce-be52-df28c59c6279"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оценка точности модели на тестовом наборе данных"
      ],
      "metadata": {
        "id": "xie7yYo41CsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_w2v = lr_model_w2v.score(X_test_w2v, y_test)\n",
        "print(\"\\nLogistic Regression Accuracy (Word2Vec):\", accuracy_w2v)\n",
        "\n",
        "accuracy_ft = lr_model_ft.score(X_test_ft, y_test)\n",
        "print(\"Logistic Regression Accuracy (FastText):\", accuracy_ft)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAjG51EAy7_U",
        "outputId": "35595158-55a1-4cbe-8aa9-8e4bbd3b8dca"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Logistic Regression Accuracy (Word2Vec): 0.9717138103161398\n",
            "Logistic Regression Accuracy (FastText): 0.9717138103161398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Из полученных данных следует следующий вывод:\n",
        "\n",
        "Word2Vec Evaluation:\n",
        "\n",
        "Модель Word2Vec дает хорошие результаты в оценке векторных представлений слов и находит семантически близкие слова, однако некоторые слова, такие как 'not' и 'no', отсутствуют в словаре, что может быть проблемой при работе с отрицаниями или отрицательными контекстами.\n",
        "\n",
        "FastText Evaluation:\n",
        "\n",
        "Модель FastText хорошо работает с векторными представлениями слов, даже при наличии опечаток или нестандартных символов благодаря использованию субслов. Метод most_similar находит схожие слова даже с опечатками или похожими субсловами.\n",
        "\n",
        "Логистическая регрессия:\n",
        "\n",
        "Модель логистической регрессии на векторных представлениях слов (Word2Vec и FastText) дает точность в 97-98% при классификации спама, что подтверждает эффективность выбранных методов предобработки и векторизации текста.\n",
        "\n"
      ],
      "metadata": {
        "id": "il0Z6NAF1Cag"
      }
    }
  ]
}